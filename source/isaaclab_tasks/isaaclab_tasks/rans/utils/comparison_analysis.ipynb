{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "This section loads and prepares the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Base directory for simulation evaluations\n",
    "BASE_SIM_DIR = \"sim_evals/\"\n",
    "\n",
    "# Predefined task and robot names\n",
    "TASKS = [\"GoToPosition\", \"GoToPose\", \"GoThroughPositions\", \"TrackVelocities\"]\n",
    "ROBOTS = [\"FloatingPlatform\", \"Kingfisher\", \"Turtlebot2\"]\n",
    "\n",
    "# Color scheme for robots (Paired palette)\n",
    "ROBOT_COLORS = {\n",
    "    \"FloatingPlatform\": sns.color_palette(\"Paired\")[5],  # Red\n",
    "    \"Kingfisher\": sns.color_palette(\"Paired\")[1],        # Blue\n",
    "    \"Turtlebot2\": sns.color_palette(\"Paired\")[3],        # Green\n",
    "}\n",
    "\n",
    "# Success thresholds\n",
    "THRESHOLDS = {\n",
    "    \"GoToPosition\": {\"epsilon_p\": 0.1},\n",
    "    \"GoToPose\": {\"epsilon_p\": 0.1, \"epsilon_theta\": np.deg2rad(10)},\n",
    "    \"GoThroughPositions\": {\"min_goals_required\": 5, \"epsilon_p\": 0.2},\n",
    "    \"TrackVelocities\": {\"tau_v\": 0.2, \"tau_w\": np.deg2rad(10)},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Helper Functions\n",
    "Here, we define utility functions to assist in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute the moving average of a data series\n",
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size) / window_size, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def load_simulation_metrics(base_dir, task):\n",
    "    \"\"\"\n",
    "    Load and organize simulation CSV results from the flattened format.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Path to the 'sim_evals/' folder.\n",
    "        task (str): Task name ('GoToPosition', 'GoToPose', 'GoThroughPositions').\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with robot names as keys and dicts of metric arrays as values.\n",
    "    \"\"\"\n",
    "    task_dir = os.path.join(base_dir, task)\n",
    "    csv_files = glob(os.path.join(task_dir, \"*.csv\"))\n",
    "\n",
    "    sim_results = {}\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        robot_name = os.path.basename(csv_file).replace(\".csv\", \"\")\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        timesteps = int(df[\"timestep\"].max()) + 1\n",
    "        num_envs = int(df[\"env_id\"].nunique())\n",
    "\n",
    "        robot_data = {}\n",
    "\n",
    "        for metric in df[\"metric\"].unique():\n",
    "            metric_df = df[df[\"metric\"] == metric].copy()\n",
    "            metric_pivot = metric_df.pivot(index=\"timestep\", columns=\"env_id\", values=\"value\")\n",
    "            metric_array = metric_pivot.values  # shape (timesteps, num_envs)\n",
    "            robot_data[metric] = metric_array\n",
    "\n",
    "        sim_results[robot_name] = robot_data\n",
    "\n",
    "        print(f\"✅ Loaded {robot_name} ({task}): {timesteps} timesteps, {num_envs} envs.\")\n",
    "\n",
    "    return sim_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Analysis Execution\n",
    "This section contains the main computational steps for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simulation_results(sim_results, task_name, thresholds, split_axes=False):\n",
    "    \"\"\"\n",
    "    Plot mean ± std curves for all robots for a given task and display a success rate table.\n",
    "    \"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    success_rates = {}\n",
    "    heading_color = sns.color_palette(\"Paired\")[7]  # Orange\n",
    "\n",
    "    if task_name == \"TrackVelocities\":\n",
    "        if split_axes:\n",
    "            # === Separate Linear Velocity Error Plot ===\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for robot, metrics in sim_results.items():\n",
    "                color = ROBOT_COLORS.get(robot, \"black\")\n",
    "                lin_vel_errors = metrics[\"linear_velocity_errors\"]\n",
    "\n",
    "                mean_lin_vel = np.mean(lin_vel_errors, axis=1)\n",
    "                std_lin_vel = np.std(lin_vel_errors, axis=1)\n",
    "                timesteps = np.arange(len(mean_lin_vel))\n",
    "\n",
    "                plt.plot(timesteps, mean_lin_vel, label=f\"{robot}\", color=color, linewidth=2)\n",
    "                plt.fill_between(timesteps, mean_lin_vel - std_lin_vel, mean_lin_vel + std_lin_vel, alpha=0.2, color=color)\n",
    "\n",
    "            plt.axhline(thresholds[task_name][\"tau_v\"], linestyle=\"--\", color=\"black\", label=\"Linear Velocity Threshold\", linewidth=1.5)\n",
    "            plt.xlabel(\"Timesteps\", fontsize=12)\n",
    "            plt.ylabel(\"Linear Velocity Error (m/s)\", fontsize=12)\n",
    "            plt.title(f\"{task_name}: Linear Velocity Tracking\", fontsize=14, fontweight=\"bold\")\n",
    "            plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "            plt.legend(loc=\"upper right\", fontsize=10, frameon=True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # === Separate Angular Velocity Error Plot ===\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for robot, metrics in sim_results.items():\n",
    "                color = ROBOT_COLORS.get(robot, \"black\")\n",
    "                ang_vel_errors = metrics[\"angular_velocity_errors\"]\n",
    "\n",
    "                mean_ang_vel = np.mean(ang_vel_errors, axis=1)\n",
    "                std_ang_vel = np.std(ang_vel_errors, axis=1)\n",
    "                timesteps = np.arange(len(mean_ang_vel))\n",
    "\n",
    "                plt.plot(timesteps, mean_ang_vel, label=f\"{robot}\", color=color, linewidth=2, linestyle=\"--\")\n",
    "                plt.fill_between(timesteps, mean_ang_vel - std_ang_vel, mean_ang_vel + std_ang_vel, alpha=0.2, color=color)\n",
    "\n",
    "            plt.axhline(thresholds[task_name][\"tau_w\"], linestyle=\"--\", color=\"orange\", label=\"Angular Velocity Threshold\", linewidth=1.5)\n",
    "            plt.xlabel(\"Timesteps\", fontsize=12)\n",
    "            plt.ylabel(\"Angular Velocity Error (rad/s)\", fontsize=12)\n",
    "            plt.title(f\"{task_name}: Angular Velocity Tracking\", fontsize=14, fontweight=\"bold\")\n",
    "            plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "            plt.legend(loc=\"upper right\", fontsize=10, frameon=True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            # === Combined Linear + Angular Velocity Tracking ===\n",
    "            fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            for robot, metrics in sim_results.items():\n",
    "                color = ROBOT_COLORS.get(robot, \"black\")\n",
    "                lin_vel_errors = metrics[\"linear_velocity_errors\"]\n",
    "                ang_vel_errors = metrics[\"angular_velocity_errors\"]\n",
    "\n",
    "                mean_lin_vel = np.mean(lin_vel_errors, axis=1)\n",
    "                std_lin_vel = np.std(lin_vel_errors, axis=1)\n",
    "                mean_ang_vel = np.mean(ang_vel_errors, axis=1)\n",
    "                std_ang_vel = np.std(ang_vel_errors, axis=1)\n",
    "                timesteps = np.arange(len(mean_lin_vel))\n",
    "\n",
    "                ax1.plot(timesteps, mean_lin_vel, label=f\"{robot} - Linear Velocity\", color=color, linewidth=2)\n",
    "                ax1.fill_between(timesteps, mean_lin_vel - std_lin_vel, mean_lin_vel + std_lin_vel, alpha=0.2, color=color)\n",
    "\n",
    "                ax2.plot(timesteps, mean_ang_vel, label=f\"{robot} - Angular Velocity\", color=color, linestyle=\"--\", linewidth=2)\n",
    "                ax2.fill_between(timesteps, mean_ang_vel - std_ang_vel, mean_ang_vel + std_ang_vel, alpha=0.2, color=color)\n",
    "\n",
    "            ax1.axhline(thresholds[task_name][\"tau_v\"], linestyle=\"--\", color=\"black\", label=\"Linear Velocity Threshold\", linewidth=1.5)\n",
    "            ax2.axhline(thresholds[task_name][\"tau_w\"], linestyle=\"--\", color=\"orange\", label=\"Angular Velocity Threshold\", linewidth=1.5)\n",
    "\n",
    "            ax1.set_xlabel(\"Timesteps\", fontsize=12)\n",
    "            ax1.set_ylabel(\"Linear Velocity Error (m/s)\", fontsize=12)\n",
    "            ax2.set_ylabel(\"Angular Velocity Error (rad/s)\", fontsize=12, color=heading_color)\n",
    "\n",
    "            ax2.tick_params(axis='y', labelcolor=heading_color)\n",
    "            \n",
    "            ax1.set_ylim(bottom=0)\n",
    "            ax2.set_ylim(bottom=0)\n",
    "            ax1.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "            ax2.grid(False)\n",
    "            handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "            handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "            plt.legend(handles1 + handles2, labels1 + labels2, loc=\"upper right\", fontsize=10, frameon=True)\n",
    "\n",
    "            plt.title(f\"{task_name}: Simulation Performance\", fontsize=14, fontweight=\"bold\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # tracking if mean velocity is within threshold for each robot\n",
    "        for robot, metrics in sim_results.items():\n",
    "\n",
    "            lin_vel_errors = metrics[\"linear_velocity_errors\"]\n",
    "            ang_vel_errors = metrics[\"angular_velocity_errors\"]\n",
    "\n",
    "            mean_lin_vel = np.mean(lin_vel_errors, axis=1)\n",
    "            mean_ang_vel = np.mean(ang_vel_errors, axis=1)\n",
    "\n",
    "            success = np.mean((np.abs(mean_lin_vel) < thresholds[task_name][\"tau_v\"]) & (np.abs(mean_ang_vel) < thresholds[task_name][\"tau_w\"]))\n",
    "            success_rates[robot] = f\"{success*100:.1f}%\"\n",
    "        \n",
    "\n",
    "    elif task_name == \"GoToPose\":\n",
    "        success = {}\n",
    "\n",
    "        if split_axes:\n",
    "            # === Separate Distance Plot ===\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for robot, metrics in sim_results.items():\n",
    "                color = ROBOT_COLORS.get(robot, \"black\")\n",
    "                distances = metrics[\"distance_to_goal\"]\n",
    "                mean_distance = np.mean(distances, axis=1)\n",
    "                std_distance = np.std(distances, axis=1)\n",
    "                timesteps = np.arange(len(mean_distance))\n",
    "\n",
    "                plt.plot(timesteps, mean_distance, label=f\"{robot}\", color=color, linewidth=2)\n",
    "                plt.fill_between(timesteps, mean_distance - std_distance, mean_distance + std_distance, alpha=0.2, color=color)\n",
    "\n",
    "            plt.axhline(thresholds[task_name][\"epsilon_p\"], linestyle=\"--\", color=\"black\", label=\"Distance Threshold (0.1m)\", linewidth=1.5)\n",
    "            plt.xlabel(\"Timesteps\", fontsize=12)\n",
    "            plt.ylabel(\"Distance to Goal (m)\", fontsize=12)\n",
    "            plt.title(f\"{task_name}: Distance Convergence\", fontsize=14, fontweight=\"bold\")\n",
    "            plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "            plt.legend(loc=\"upper right\", fontsize=10, frameon=True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # === Separate Heading Plot ===\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for robot, metrics in sim_results.items():\n",
    "                color = ROBOT_COLORS.get(robot, \"black\")\n",
    "                headings = np.rad2deg(metrics[\"heading_errors\"])\n",
    "                mean_heading = np.mean(headings, axis=1)\n",
    "                std_heading = np.std(headings, axis=1)\n",
    "                timesteps = np.arange(len(mean_heading))\n",
    "\n",
    "                plt.plot(timesteps, mean_heading, label=f\"{robot}\", color=color, linewidth=2, linestyle=\"--\")\n",
    "                plt.fill_between(timesteps, mean_heading - std_heading, mean_heading + std_heading, alpha=0.2, color=color)\n",
    "\n",
    "            plt.axhline(np.rad2deg(thresholds[task_name][\"epsilon_theta\"]), linestyle=\"--\", color=\"orange\", label=\"Heading Threshold (10°)\", linewidth=1.5)\n",
    "            plt.xlabel(\"Timesteps\", fontsize=12)\n",
    "            plt.ylabel(\"Heading Error (°)\", fontsize=12)\n",
    "            plt.title(f\"{task_name}: Heading Alignment\", fontsize=14, fontweight=\"bold\")\n",
    "            plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "            plt.legend(loc=\"upper right\", fontsize=10, frameon=True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            # === Combined Distance + Heading Dual-Axis Plot ===\n",
    "            fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            for robot, metrics in sim_results.items():\n",
    "                color = ROBOT_COLORS.get(robot, \"black\")\n",
    "\n",
    "                distances = metrics[\"distance_to_goal\"]\n",
    "                headings = metrics[\"heading_errors\"]\n",
    "\n",
    "                mean_distance = np.mean(distances, axis=1)\n",
    "                std_distance = np.std(distances, axis=1)\n",
    "                mean_heading = np.rad2deg(np.mean(headings, axis=1))\n",
    "                std_heading = np.rad2deg(np.std(headings, axis=1))\n",
    "                timesteps = np.arange(len(mean_distance))\n",
    "\n",
    "                ax1.plot(timesteps, mean_distance, label=f\"{robot} - Distance\", color=color, linewidth=2)\n",
    "                ax1.fill_between(timesteps, mean_distance - std_distance, mean_distance + std_distance, alpha=0.2, color=color)\n",
    "\n",
    "                ax2.plot(timesteps, mean_heading, label=f\"{robot} - Heading\", color=color, linestyle=\"--\", linewidth=2)\n",
    "                ax2.fill_between(timesteps, mean_heading - std_heading, mean_heading + std_heading, alpha=0.2, color=color)\n",
    "\n",
    "            ax1.axhline(thresholds[task_name][\"epsilon_p\"], linestyle=\"--\", color=\"red\", label=\"Distance Threshold (0.1m)\", linewidth=1.5)\n",
    "            ax2.axhline(np.rad2deg(thresholds[task_name][\"epsilon_theta\"]), linestyle=\"--\", color=\"orange\", label=\"Heading Threshold (10°)\", linewidth=1.5)\n",
    "\n",
    "            ax1.set_xlabel(\"Timesteps\", fontsize=12)\n",
    "            ax1.set_ylabel(\"Distance to Goal (m)\", fontsize=12)\n",
    "            ax2.set_ylabel(\"Heading Error (°)\", fontsize=12, color=\"orange\")\n",
    "            ax1.set_ylim(bottom=0)\n",
    "            ax2.set_ylim(bottom=0)\n",
    "            ax1.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "\n",
    "            handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "            handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "            plt.legend(handles1 + handles2, labels1 + labels2, loc=\"upper right\", fontsize=10, frameon=True)\n",
    "\n",
    "            plt.title(f\"{task_name}: Simulation Performance\", fontsize=14, fontweight=\"bold\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        for robot, metrics in sim_results.items():\n",
    "            distances = metrics[\"distance_to_goal\"]\n",
    "            headings = metrics[\"heading_errors\"]\n",
    "\n",
    "            mean_distance = np.mean(distances, axis=1)\n",
    "            mean_heading = np.rad2deg(np.mean(headings, axis=1))\n",
    "            # save success for distance and heading separately for each robot\n",
    "            success_distance = np.mean(mean_distance < thresholds[task_name][\"epsilon_p\"])\n",
    "            success_heading = np.mean(mean_heading < thresholds[task_name][\"epsilon_theta\"])\n",
    "            success_rates[robot] = f\"{success_distance*100:.1f}% / {success_heading*100:.1f}%\"\n",
    "        \n",
    "            \n",
    "    elif task_name == \"GoThroughPositions\":\n",
    "        # Cumulative Goals Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        min_goals_required = thresholds[task_name][\"min_goals_required\"]\n",
    "        \n",
    "        for robot, metrics in sim_results.items():\n",
    "            color = ROBOT_COLORS.get(robot, \"black\")\n",
    "            cumulative_goals = metrics[\"cumulative_goals\"]\n",
    "            timesteps = np.arange(cumulative_goals.shape[0])\n",
    "\n",
    "            mean_goals = np.mean(cumulative_goals, axis=1)\n",
    "            std_goals = np.std(cumulative_goals, axis=1)\n",
    "\n",
    "            # Optional smoothing\n",
    "            window_size = 20\n",
    "            smooth_mean_goals = moving_average(mean_goals, window_size)\n",
    "            smooth_std_goals = moving_average(std_goals, window_size)\n",
    "            smooth_timesteps = timesteps[:len(smooth_mean_goals)]\n",
    "\n",
    "            plt.plot(smooth_timesteps, smooth_mean_goals, label=f\"{robot}\", color=color, linewidth=2)\n",
    "            plt.fill_between(\n",
    "                smooth_timesteps,\n",
    "                smooth_mean_goals - smooth_std_goals,\n",
    "                smooth_mean_goals + smooth_std_goals,\n",
    "                alpha=0.2,\n",
    "                color=color\n",
    "            )\n",
    "\n",
    "            # Success rate: percentage of envs with at least `min_goals_required`\n",
    "            final_goals_per_env = cumulative_goals[-1, :]\n",
    "            success = np.mean(final_goals_per_env >= min_goals_required)\n",
    "            success_rates[robot] = f\"{success * 100:.1f}%\"\n",
    "\n",
    "        plt.xlabel(\"Timesteps\", fontsize=12)\n",
    "        plt.ylabel(\"Cumulative Goals Reached\", fontsize=12)\n",
    "        plt.title(f\"{task_name}: Cumulative Goals Over Time\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "        plt.legend(loc=\"upper left\", fontsize=10, frameon=True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        TOTAL_ENVS = 4096  \n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for robot, metrics in sim_results.items():\n",
    "            color = ROBOT_COLORS.get(robot, \"black\")\n",
    "            final_goals = metrics[\"cumulative_goals\"][-1, :]\n",
    "\n",
    "            sns.histplot(\n",
    "                final_goals,\n",
    "                bins=range(int(final_goals.min()), int(final_goals.max()) + 1),  # Keep integer bins\n",
    "                kde=True,  \n",
    "                kde_kws={\"bw_adjust\": 2.5},  # Adjust KDE to avoid multiple peaks\n",
    "                stat=\"percent\",  # Converts the histogram into percentage\n",
    "                color=color,\n",
    "                label=f\"{robot}\",\n",
    "                alpha=0.5,\n",
    "                edgecolor=\"black\",\n",
    "                linewidth=0.6\n",
    "            )\n",
    "\n",
    "        # Customize Y-axis to explicitly show percentages\n",
    "        plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f\"{y:.1f}%\"))\n",
    "\n",
    "        # Enhance plot aesthetics\n",
    "        plt.xlabel(\"Total Goals Reached\", fontsize=12)\n",
    "        plt.ylabel(\"Percentage of Environments\", fontsize=12)\n",
    "        plt.title(f\"{task_name}: Distribution of Goals Reached per Environment\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "        plt.legend(loc=\"upper right\", fontsize=10, frameon=True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for robot, metrics in sim_results.items():\n",
    "            color = ROBOT_COLORS.get(robot, \"black\")\n",
    "\n",
    "            if task_name == \"GoToPosition\":\n",
    "                data = metrics[\"distance_to_goal\"]\n",
    "                mean_distance = np.mean(data, axis=1)\n",
    "                std_distance = np.std(data, axis=1)\n",
    "                timesteps = np.arange(len(mean_distance))\n",
    "\n",
    "                plt.plot(timesteps, mean_distance, label=f\"{robot}\", color=color, linewidth=2)\n",
    "                plt.fill_between(timesteps, mean_distance - std_distance, mean_distance + std_distance, alpha=0.2, color=color)\n",
    "\n",
    "                final_distances = data[-1, :]\n",
    "                success = np.mean(final_distances < thresholds[task_name][\"epsilon_p\"])\n",
    "                success_rates[robot] = f\"{success*100:.1f}%\"\n",
    "\n",
    "            elif task_name == \"GoThroughPositions\":\n",
    "                success_rate = metrics[\"waypoint_success_rate\"]\n",
    "                mean_success = np.mean(success_rate, axis=1)\n",
    "                std_success = np.std(success_rate, axis=1)\n",
    "                timesteps = np.arange(len(mean_success))\n",
    "\n",
    "                plt.plot(timesteps, mean_success, label=f\"{robot}\", color=color, linewidth=2)\n",
    "                plt.fill_between(timesteps, mean_success - std_success, mean_success + std_success, alpha=0.2, color=color)\n",
    "\n",
    "                final_success = np.mean(success_rate[-1, :])\n",
    "                success_rates[robot] = f\"{final_success*100:.1f}%\"\n",
    "\n",
    "        if task_name == \"GoToPosition\":\n",
    "            plt.axhline(thresholds[\"GoToPosition\"][\"epsilon_p\"], linestyle=\"--\", color=\"black\", label=\"Threshold (0.1m)\", linewidth=1.5)\n",
    "\n",
    "        plt.xlabel(\"Timesteps\", fontsize=12)\n",
    "        ylabel = {\n",
    "            \"GoToPosition\": \"Distance to Goal (m)\",\n",
    "            \"GoThroughPositions\": \"Success Rate\"\n",
    "        }\n",
    "        plt.ylabel(ylabel[task_name], fontsize=12)\n",
    "        plt.title(f\"{task_name}: Simulation Performance\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "        plt.legend(loc=\"upper right\", fontsize=10, frameon=True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    # Success rate table\n",
    "    success_table = pd.DataFrame.from_dict(success_rates, orient=\"index\", columns=[\"Success Rate\"])\n",
    "    success_table.index.name = \"Robot\"\n",
    "    success_table[\"Task\"] = task_name\n",
    "    display(success_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Results\n",
    "We generate plots and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_SIM_DIR = \"../../../../../sim_evals/\"\n",
    "TASK = \"GoThroughPositions\"\n",
    "sim_results = load_simulation_metrics(BASE_SIM_DIR, TASK)\n",
    "plot_simulation_results(sim_results, TASK, THRESHOLDS, split_axes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for Field Tests Experiments\n",
    "load_field_tests: Loads and selects top-N field test runs for a robot-task pair. \n",
    "\n",
    "aggregate_field_runs: Aggregate top-N runs into mean and std arrays over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "def load_field_tests(base_dir, robot, task, top_n=5, metric=\"distance_error.m\"):\n",
    "    \"\"\"\n",
    "    Load and select top-N field test runs for a robot-task pair.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Path to 'field_tests_data/'.\n",
    "        robot (str): Robot name.\n",
    "        task (str): Task name.\n",
    "        top_n (int): Number of best runs to select.\n",
    "        metric (str): Metric to sort runs by (e.g., \"distance_error.m\").\n",
    "\n",
    "    Returns:\n",
    "        list: List of top-N DataFrames (one per run).\n",
    "    \"\"\"\n",
    "    folder_path = os.path.join(base_dir, robot, task)\n",
    "    csv_files = glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "    runs = []\n",
    "    scores = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        # save runs name\n",
    "        df['run_id'] = file.split('/')[-1].split('.')[0]\n",
    "        if metric in df.columns:\n",
    "            final_value = df[metric].iloc[-1]\n",
    "            runs.append(df)\n",
    "            scores.append(final_value)\n",
    "\n",
    "    if not runs:\n",
    "        print(f\"⚠️ No valid runs found for {robot} - {task}.\")\n",
    "        return []\n",
    "\n",
    "    # Select top-N based on lowest final metric value\n",
    "    top_indices = np.argsort(scores)[:top_n]\n",
    "    top_runs = [runs[i] for i in top_indices]\n",
    "\n",
    "    print(f\"✅ Loaded {len(top_runs)} top runs for {robot} - {task}.\")\n",
    "    return top_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_field_runs(runs, metric):\n",
    "    \"\"\"\n",
    "    Aggregate top-N runs into mean and std arrays over time.\n",
    "\n",
    "    Args:\n",
    "        runs (list): List of DataFrames.\n",
    "        metric (str): Metric to extract.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): Mean and std arrays over time.\n",
    "    \"\"\"\n",
    "    all_runs = [run[metric].values for run in runs]\n",
    "    min_length = min(len(run) for run in all_runs)\n",
    "\n",
    "    # Trim runs to the shortest length to align\n",
    "    trimmed_runs = [run[:min_length] for run in all_runs]\n",
    "    data = np.vstack(trimmed_runs)\n",
    "\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "\n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_field_success_rate(runs, task, thresholds):\n",
    "    \"\"\"\n",
    "    Compute success rate over top-N runs.\n",
    "    \"\"\"\n",
    "    success_count = 0\n",
    "    for run in runs:\n",
    "        if task == \"GoToPosition\":\n",
    "            if run[\"distance_error.m\"].iloc[-1] < thresholds[task][\"epsilon_p\"]:\n",
    "                success_count += 1\n",
    "        elif task == \"GoToPose\":\n",
    "            if (run[\"distance_error.m\"].iloc[-1] < thresholds[task][\"epsilon_p\"] and\n",
    "                abs(run[\"heading_error.rad\"].iloc[-1]) < thresholds[task][\"epsilon_theta\"]):\n",
    "                success_count += 1\n",
    "        elif task == \"GoThroughPositions\":\n",
    "            if run[\"num_goals_reached.u\"].max() > 0:\n",
    "                success_count += 1\n",
    "    print(runs[0][\"distance_error.m\"])\n",
    "\n",
    "    return f\"{(success_count / len(runs)) * 100:.1f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cumulative_goals_field(run, threshold=0.2, robot=\"Kingfisher\"):\n",
    "    \"\"\"\n",
    "    Compute cumulative goals reached for a single field test run, handling column swaps for specific robots.\n",
    "\n",
    "    Args:\n",
    "        run (pd.DataFrame): Field test run dataframe.\n",
    "        robot (str): Name of the robot (used for column correction).\n",
    "        threshold (float): Distance threshold to consider a goal as reached.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Cumulative goals reached over time.\n",
    "    \"\"\"\n",
    "    # Detect the correct column based on the robot type\n",
    "    # if robot == \"Kingfisher\":\n",
    "    #     distance_column = \"linear_velocities_bodyx.m/s\"  # Fix the swap\n",
    "    # else:\n",
    "    distance_column = \"task_data.dist.m\"\n",
    "\n",
    "    # Ensure the column exists before proceeding\n",
    "    if distance_column not in run.columns:\n",
    "        raise ValueError(f\"Column '{distance_column}' not found in the field test data for {robot}\")\n",
    "\n",
    "    # simply return num_goals_reached\n",
    "    # return run[\"num_goals_reached.u\"].values\n",
    "    distance_column = \"task_data.dist.m\"\n",
    "    distances = run[distance_column].values\n",
    "\n",
    "    goals_reached = run[\"num_goals_reached.u\"].values\n",
    "    tot_goals = 0\n",
    "    cumulative_goals = np.zeros_like(distances)\n",
    "\n",
    "    for t in range(len(distances)):\n",
    "        #if turtlebot2, we need to add 1 to the total goals reached\n",
    "        if robot == \"Turtlebot2\":\n",
    "            goals_reached[t] += 1\n",
    "        if distances[0] < threshold and goals_reached[t] > tot_goals:\n",
    "            tot_goals = goals_reached[t]  # update total goals reached\n",
    "        cumulative_goals[t] = tot_goals\n",
    "\n",
    "    return cumulative_goals\n",
    "\n",
    "    # for t in range(len(distances)):\n",
    "    #     if distances[t] < threshold and not goal_reached_flag:\n",
    "    #         total_goals += 1\n",
    "    #         goal_reached_flag = True\n",
    "    #     elif distances[t] > threshold:\n",
    "    #         goal_reached_flag = False\n",
    "    #     cumulative_goals[t] = total_goals\n",
    "\n",
    "    # return cumulative_goals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot function for the field test, following the same scheme for simulation results\n",
    "We generate plots and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_field_test_results(top_runs, task, thresholds, robot, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot mean ± std curves for field test top-N runs with success rate and twin axes for GoToPose.\n",
    "    \"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    color_distance = ROBOT_COLORS.get(robot, \"black\")\n",
    "    color_heading = sns.color_palette(\"Paired\")[7]\n",
    "    color_goals = sns.color_palette(\"Paired\")[2]\n",
    "    color_goals_fill = sns.color_palette(\"Paired\")[3]\n",
    "    threshold_distance = \"black\"\n",
    "    threshold_heading = \"orange\"\n",
    "\n",
    "    timesteps = np.arange(min(len(run) for run in top_runs))\n",
    "    print(f\"[INFO] Run lengths for {robot} - {task}: {[len(run) for run in top_runs]}\")\n",
    "\n",
    "    if task == \"GoToPosition\":\n",
    "        mean_distance, std_distance = aggregate_field_runs(top_runs, \"distance_error.m\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.set_xlabel(\"Timesteps\")\n",
    "        ax.set_ylabel(\"Distance to Goal (m)\", color=color_distance)\n",
    "        ax.plot(timesteps, mean_distance, label=\"Mean Distance Error\", color=color_distance, linewidth=2)\n",
    "        ax.fill_between(timesteps, mean_distance - std_distance, mean_distance + std_distance, alpha=0.2, color=color_distance)\n",
    "        ax.axhline(thresholds[task][\"epsilon_p\"], linestyle=\"--\", color=threshold_distance, label=\"Distance Threshold (0.1m)\", linewidth=1.5)\n",
    "        ax.tick_params(axis='y', labelcolor=color_distance)\n",
    "        ax.set_ylim(bottom=0)\n",
    "        ax.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "        plt.title(f\"{robot} - {task} Field Test Performance\", fontsize=14, fontweight=\"bold\")\n",
    "        fig.tight_layout()\n",
    "        ax.legend(loc=\"upper right\", fontsize=10, frameon=True)\n",
    "        if save_path:\n",
    "            plt.savefig(os.path.join(save_path, f\"{robot}_{task}_field_results.png\"), dpi=200)\n",
    "        plt.show()\n",
    "\n",
    "    elif task == \"GoToPose\":\n",
    "        mean_distance, std_distance = aggregate_field_runs(top_runs, \"distance_error.m\")\n",
    "        mean_heading, std_heading = aggregate_field_runs(top_runs, \"heading_error.rad\")\n",
    "        mean_heading = np.abs(np.rad2deg(mean_heading))\n",
    "        std_heading = np.abs(np.rad2deg(std_heading))\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "        ax1.set_xlabel(\"Timesteps\")\n",
    "        ax1.set_ylabel(\"Distance Error (m)\", color=color_distance)\n",
    "        ax1.plot(timesteps, mean_distance, label=\"Mean Distance Error\", color=color_distance, linewidth=2)\n",
    "        ax1.fill_between(timesteps, mean_distance - std_distance, mean_distance + std_distance, alpha=0.2, color=color_distance)\n",
    "        ax1.axhline(thresholds[task][\"epsilon_p\"], linestyle=\"--\", color=threshold_distance, label=\"Distance Threshold (0.1m)\", linewidth=1.5)\n",
    "        ax1.tick_params(axis='y', labelcolor=color_distance)\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylabel(\"Heading Error (°)\", color=color_heading)\n",
    "        ax2.plot(timesteps, mean_heading, label=\"Mean Heading Error\", color=color_heading, linestyle=\"--\", linewidth=2)\n",
    "        ax2.fill_between(timesteps, mean_heading - std_heading, mean_heading + std_heading, alpha=0.2, color=color_heading)\n",
    "        ax2.axhline(np.rad2deg(thresholds[task][\"epsilon_theta\"]), linestyle=\"--\", color=threshold_heading, label=\"Heading Threshold (10°)\", linewidth=1.5)\n",
    "        ax2.tick_params(axis='y', labelcolor=color_heading)\n",
    "\n",
    "        ax1.set_ylim(bottom=0)\n",
    "        ax2.set_ylim(bottom=0)\n",
    "\n",
    "        # Only enable grid for the primary y-axis (distance error)\n",
    "        ax1.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "        ax2.grid(False)  # Disable grid for the secondary y-axis (heading error)\n",
    "\n",
    "        plt.title(f\"{robot} - {task} Field Test Performance\", fontsize=14, fontweight=\"bold\")\n",
    "        fig.tight_layout()\n",
    "\n",
    "        handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "        handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(handles1 + handles2, labels1 + labels2, loc=\"upper right\", fontsize=10, frameon=True)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(os.path.join(save_path, f\"{robot}_{task}_field_results.png\"), dpi=200)\n",
    "        plt.show()\n",
    "        \n",
    "    elif task == \"GoThroughPositions\":\n",
    "        # Compute cumulative goals for all runs\n",
    "        cumulative_goals_runs = []\n",
    "        final_goals_per_run = []\n",
    "        for run in top_runs:\n",
    "            cumulative_goals = compute_cumulative_goals_field(run, threshold=thresholds[task][\"epsilon_p\"])\n",
    "            cumulative_goals_runs.append(cumulative_goals)\n",
    "            final_goals_per_run.append(cumulative_goals[-1])  # Take final goal count\n",
    "\n",
    "        # Handle variable-length runs by trimming to the shortest\n",
    "        min_length = min(len(cg) for cg in cumulative_goals_runs)\n",
    "        cumulative_goals_runs = np.array([cg[:min_length] for cg in cumulative_goals_runs])\n",
    "        timesteps = np.arange(min_length)\n",
    "\n",
    "        # Aggregate mean and std\n",
    "        mean_goals = np.mean(cumulative_goals_runs, axis=0)\n",
    "        std_goals = np.std(cumulative_goals_runs, axis=0)\n",
    "\n",
    "        # Optional smoothing\n",
    "        window_size = 100\n",
    "        smooth_mean_goals = moving_average(mean_goals, window_size)\n",
    "        smooth_std_goals = moving_average(std_goals, window_size)\n",
    "        smooth_timesteps = timesteps[:len(smooth_mean_goals)]\n",
    "\n",
    "        # Plot cumulative goals over time\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.set_xlabel(\"Timesteps\")\n",
    "        ax.set_ylabel(\"Cumulative Goals Reached\", fontsize=12)\n",
    "        ax.plot(smooth_timesteps, smooth_mean_goals, label=\"Smoothed Mean Goals Reached\", color=color_goals, linewidth=2)\n",
    "        ax.fill_between(smooth_timesteps, smooth_mean_goals - smooth_std_goals, smooth_mean_goals + smooth_std_goals, alpha=0.2, color=color_goals_fill)\n",
    "        ax.set_ylim(bottom=0)\n",
    "        ax.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "        plt.title(f\"{robot} - {task} Field Test Performance\", fontsize=14, fontweight=\"bold\")\n",
    "        fig.tight_layout()\n",
    "        ax.legend(loc=\"upper left\", fontsize=10, frameon=True)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(os.path.join(save_path, f\"{robot}_{task}_field_results_cumulative_goals.png\"), dpi=200)\n",
    "        plt.show()\n",
    "\n",
    "        # Plot histogram with KDE overlay\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        # Histogram with density normalization for smoothness\n",
    "        sns.histplot(final_goals_per_run, binwidth=1, color=\"blue\", kde=False, stat=\"density\", alpha=0.6)\n",
    "        # KDE for smooth interpolation\n",
    "        sns.kdeplot(final_goals_per_run, bw_adjust=0.5, color=\"red\", linewidth=2, alpha=0.8)\n",
    "        # Labels and customization\n",
    "        ax.set_xlabel(\"Total Goals Reached\", fontsize=12)\n",
    "        ax.set_ylabel(\"Density\", fontsize=12)\n",
    "        ax.set_xlim(left=0)\n",
    "        ax.set_xticks(range(int(max(final_goals_per_run)) + 1))\n",
    "        ax.set_title(f\"{robot} - {task}: Goals Reached Distribution\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "\n",
    "\n",
    "    # Save figure if needed\n",
    "    if save_path:\n",
    "        plt.savefig(os.path.join(save_path, f\"{robot}_{task}_field_results_goals_histogram.png\"), dpi=200)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Summary Statistics\n",
    "\n",
    "We allow the selection of task, robot pairs, to load all the runs present in the field_test experiment folders and plot the results.\n",
    "\n",
    "The tests can be filtered based on the best top_n runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FIELD_DIR = \"../../../../../field_tests_data\"\n",
    "\n",
    "robot = \"FloatingPlatform\"\n",
    "task = \"GoToPose\"\n",
    "\n",
    "all_runs = load_field_tests(BASE_FIELD_DIR, robot, task, top_n=20)\n",
    "plot_field_test_results(all_runs, task, THRESHOLDS, robot)\n",
    "# Success rate\n",
    "success_rate = compute_field_success_rate(all_runs, task, THRESHOLDS)\n",
    "success_table = pd.DataFrame({robot: [success_rate]}, index=[\"Success Rate\"])\n",
    "display(success_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional filtering and test comparisons\n",
    "\n",
    "In the following experiments are filtered by length or specific performance thresholds to see the difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_runs = [run for run in all_runs if len(run) > 2000]\n",
    "short_runs = [run for run in all_runs if len(run) <= 1000]\n",
    "medium_runs = [run for run in all_runs if 1000 < len(run) <= 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bad runs and print their tot number and names\n",
    "bad_runs = []\n",
    "good_runs = []\n",
    "for run in all_runs:\n",
    "    if run[\"distance_error.m\"].iloc[-1] > 2 * THRESHOLDS[task][\"epsilon_p\"]:\n",
    "        bad_runs.append(run)\n",
    "    else:\n",
    "        good_runs.append(run)\n",
    "\n",
    "print(f\"Found {len(bad_runs)} bad runs for {robot} - {task} and {len(good_runs)} good runs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the good runs\n",
    "plot_field_test_results(good_runs, task, THRESHOLDS, robot)\n",
    "# Success rate\n",
    "success_rate = compute_field_success_rate(good_runs, task, THRESHOLDS)\n",
    "success_table = pd.DataFrame({robot: [success_rate]}, index=[\"Success Rate\"])\n",
    "display(success_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using effective experiment time (s) instead of timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gotoposition_comparison_timesteps(all_robot_runs, thresholds, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot mean ± std curves for GoToPosition task comparing multiple robots, using timesteps as the x-axis.\n",
    "\n",
    "    Args:\n",
    "        all_robot_runs (dict): Dictionary with robot names as keys and lists of runs (DataFrames) as values.\n",
    "        thresholds (dict): Dictionary of success thresholds for GoToPosition.\n",
    "        save_path (str, optional): Directory to save plots.\n",
    "    \"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for robot, runs in all_robot_runs.items():\n",
    "        color = ROBOT_COLORS.get(robot, \"black\")\n",
    "        distance_runs = []\n",
    "\n",
    "        for run in runs:\n",
    "            if isinstance(run, pd.DataFrame):\n",
    "                if \"distance_error.m\" not in run.columns:\n",
    "                    print(f\"⚠️ Skipping {robot}: Missing required columns\")\n",
    "                    continue\n",
    "                distance_runs.append(run[\"distance_error.m\"].values)\n",
    "            else:\n",
    "                print(f\"⚠️ Skipping {robot}: Run is not a DataFrame\")\n",
    "                continue\n",
    "\n",
    "        if not distance_runs:\n",
    "            print(f\"⚠️ No valid runs for {robot}\")\n",
    "            continue\n",
    "\n",
    "        # Find the minimum run length to align all trajectories\n",
    "        min_length = min(len(run) for run in distance_runs)\n",
    "        trimmed_runs = np.array([run[:min_length] for run in distance_runs])\n",
    "\n",
    "        # Compute mean and std\n",
    "        mean_distance = np.mean(trimmed_runs, axis=0)\n",
    "        std_distance = np.std(trimmed_runs, axis=0)\n",
    "        timesteps = np.arange(min_length)\n",
    "\n",
    "        # Plot mean ± std\n",
    "        plt.plot(timesteps, mean_distance, label=f\"{robot}\", color=color, linewidth=2)\n",
    "        plt.fill_between(\n",
    "            timesteps,\n",
    "            mean_distance - std_distance,\n",
    "            mean_distance + std_distance,\n",
    "            alpha=0.2,\n",
    "            color=color\n",
    "        )\n",
    "\n",
    "    # Threshold line\n",
    "    plt.axhline(thresholds[\"GoToPosition\"][\"epsilon_p\"], linestyle=\"--\", color=\"black\", label=\"Distance Threshold (0.1m)\", linewidth=1.5)\n",
    "\n",
    "    # Labels and aesthetics\n",
    "    plt.xlabel(\"Timesteps\", fontsize=12)\n",
    "    plt.ylabel(\"Distance to Goal (m)\", fontsize=12)\n",
    "    plt.title(\"GoToPosition: Comparison Across Robots\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "    plt.legend(loc=\"upper right\", fontsize=10, frameon=True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot if requested\n",
    "    if save_path:\n",
    "        plt.savefig(os.path.join(save_path, \"GoToPosition_comparison_timesteps.png\"), dpi=200)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_in_use = \"GoToPosition\"\n",
    "all_robot_runs_gotoposition = {\n",
    "    \"FloatingPlatform\": load_field_tests(BASE_FIELD_DIR, \"FloatingPlatform\", task_in_use, top_n=20),\n",
    "    \"Kingfisher\": good_runs,\n",
    "    \"Turtlebot2\": load_field_tests(BASE_FIELD_DIR, \"Turtlebot2\", task_in_use, top_n=20),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gotoposition_comparison_timesteps(all_robot_runs_gotoposition, THRESHOLDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_runs_by_length(all_runs, min_length, max_length):\n",
    "    \"\"\"\n",
    "    Filters runs by their trajectory length.\n",
    "    \"\"\"\n",
    "    return [run for run in all_runs if min_length <= len(run) <= max_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length range for medium runs\n",
    "MIN_LENGTH = 0\n",
    "MAX_LENGTH = 1000\n",
    "threshold = THRESHOLDS[\"GoThroughPositions\"][\"epsilon_p\"]\n",
    "\n",
    "robots = [\"FloatingPlatform\", \"Kingfisher\", \"Turtlebot2\"]\n",
    "filtered_runs = {}\n",
    "\n",
    "# Filter and compute cumulative goals\n",
    "for robot in robots:\n",
    "    all_runs = load_field_tests(BASE_FIELD_DIR, robot, \"GoThroughPositions\", top_n=20)\n",
    "    runs_in_range = filter_runs_by_length(all_runs, MIN_LENGTH, MAX_LENGTH)\n",
    "    print(f\"✅ {robot}: {len(runs_in_range)} runs in range [{MIN_LENGTH}, {MAX_LENGTH}]\")\n",
    "    \n",
    "    cumulative_goals_runs = []\n",
    "    for run in runs_in_range:\n",
    "        cumulative_goals = compute_cumulative_goals_field(run, threshold, robot)\n",
    "        cumulative_goals_runs.append(cumulative_goals)\n",
    "    \n",
    "    filtered_runs[robot] = cumulative_goals_runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align lengths\n",
    "valid_runs = {robot: runs for robot, runs in filtered_runs.items() if runs}  # Remove empty entries\n",
    "if not valid_runs:\n",
    "    raise ValueError(\"No valid runs in the selected length range.\")\n",
    "\n",
    "min_timesteps = min(min(len(run) for run in runs) for runs in valid_runs.values())\n",
    "timesteps = np.arange(min_timesteps)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for robot, runs in filtered_runs.items():\n",
    "    color = ROBOT_COLORS.get(robot, \"black\")\n",
    "    goals_matrix = np.array([run[:min_timesteps] for run in runs])\n",
    "    \n",
    "    mean_goals = np.mean(goals_matrix, axis=0)\n",
    "    std_goals = np.std(goals_matrix, axis=0)\n",
    "    \n",
    "    window_size = 20\n",
    "    smooth_mean = moving_average(mean_goals, window_size)\n",
    "    smooth_std = moving_average(std_goals, window_size)\n",
    "    smooth_timesteps = timesteps[:len(smooth_mean)]\n",
    "\n",
    "    plt.plot(smooth_timesteps, smooth_mean, label=f\"{robot}\", color=color, linewidth=2)\n",
    "    plt.fill_between(\n",
    "        smooth_timesteps,\n",
    "        smooth_mean - smooth_std,\n",
    "        smooth_mean + smooth_std,\n",
    "        alpha=0.2,\n",
    "        color=color\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Timesteps\", fontsize=12)\n",
    "plt.ylabel(\"Cumulative Goals Reached\", fontsize=12)\n",
    "plt.title(\"GoThroughPositions: Field Tests Cumulative Goals\", fontsize=14, fontweight=\"bold\")\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "plt.legend(loc=\"upper left\", fontsize=10, frameon=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional plot for TrackVelocity task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_track_velocity_trajectories(csv_folder, task=\"TrackVelocity\", save_path=None):\n",
    "    \"\"\"\n",
    "    Loads trajectory data from multiple CSV files and plots the target trajectory \n",
    "    along with actual robot trajectories for the TrackVelocity task.\n",
    "\n",
    "    Args:\n",
    "        csv_folder (str): Path to the folder containing CSV trajectory files.\n",
    "        task (str): Task name, default is \"TrackVelocity\".\n",
    "        save_path (str, optional): Path to save the figure. If None, the plot is displayed.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))  # Ensure a square figure\n",
    "    ax.set_xlabel(\"X Position (m)\")\n",
    "    ax.set_ylabel(\"Y Position (m)\")\n",
    "    ax.set_title(f\"{task} Field Test Trajectories\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # Load Seaborn Paired color palette\n",
    "    colors = sns.color_palette(\"Paired\", 12)  # Use 12 distinct colors\n",
    "\n",
    "    # Load and plot each trajectory\n",
    "    csv_files = sorted([f for f in os.listdir(csv_folder) if f.endswith(\".csv\")])\n",
    "    for i, file in enumerate(csv_files):\n",
    "        file_path = os.path.join(csv_folder, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Extract world position (X, Y) for actual trajectory\n",
    "        x, y = df[\"position_world.x.m\"], df[\"position_world.y.m\"]\n",
    "\n",
    "        # Assign colors and plot trajectory\n",
    "        color = colors[i % len(colors)]  # Cycle colors if there are more than 12 files\n",
    "        ax.plot(x, y, label=\"Actual trajectory\", color=color, linewidth=2)\n",
    "        ax.scatter(x.iloc[0], y.iloc[0], marker=\"o\", color=color, edgecolors=\"black\", s=80, label=None)  # Start point\n",
    "\n",
    "        # Extract unique target positions (goal locations)\n",
    "        goals = df[[\"target_position.x.m\", \"target_position.y.m\"]].drop_duplicates()\n",
    "        ax.scatter(goals[\"target_position.x.m\"], goals[\"target_position.y.m\"], \n",
    "                   color=\"black\", marker=\"o\", s=10, label=\"Goal trajectory\" if i == 0 else None)  # Label only once\n",
    "\n",
    "    # Ensure a balanced (square) aspect ratio\n",
    "    ax.set_aspect('equal', adjustable='datalim')\n",
    "\n",
    "    ax.legend(loc=\"best\", fontsize=10, frameon=True)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(os.path.join(save_path, f\"{task}_field_trajectories.png\"), dpi=200)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_folder = BASE_FIELD_DIR + \"/Kingfisher/TrackVelocities\"\n",
    "plot_track_velocity_trajectories(csv_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
